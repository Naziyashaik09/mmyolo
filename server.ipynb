{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f455b84-3f4b-4f22-aa23-ad532b7d11e1",
   "metadata": {},
   "source": [
    "# Enter the dataset paths , the percentage for the data split and the parameters to train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ead2554-8922-48af-bc3d-ef16d064d215",
   "metadata": {},
   "source": [
    "### Before starting , once refer the readme.md file in the location \"/home/viso/sample_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f86f2872-a88c-40c6-beab-672c0ea28dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the dataset root path /home/viso/sample_dataset\n",
      "Enter the annotation path ,(json file) /home/viso/sample_dataset/annotations/total_annotations.json\n",
      "Enter the percentage to divide the train dataset. Eg:0.8 0.8\n",
      "Enter the percentage to divide the test dataset. Eg:0.1 0.1\n",
      "Enter the percentage to divide the validation dataset. Eg:0.1 0.1.\n",
      "Enter the MAX_EPOCHS  100\n",
      "Enter the BATCH_SIZE 4\n"
     ]
    }
   ],
   "source": [
    "# dataset_root is the path , where images and the annotation file consists \n",
    "dataset_root=input('Enter the dataset root path')\n",
    "\n",
    "# annotation_path , where there is a single file annotation in the coco (json) format \n",
    "annotation_path=input('Enter the annotation path ,(json file)')\n",
    "\n",
    "# consists of images for training\n",
    "train_percentage=input('Enter the percentage to divide the train dataset. Eg:0.8')\n",
    "test_percentage= input('Enter the percentage to divide the test dataset. Eg:0.1')\n",
    "val_percentage= input('Enter the percentage to divide the validation dataset. Eg:0.1')\n",
    "\n",
    "# enter the epochs , upto how much epochs the models needs to train \n",
    "MAX_EPOCHS = input('Enter the MAX_EPOCHS ')\n",
    "\n",
    "# BATCH_SIZE means  \"The number of training examples utilized in one iteration\".\n",
    "BATCH_SIZE = input('Enter the BATCH_SIZE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec6373b-a8bb-4491-a1c4-74bc87e31066",
   "metadata": {},
   "source": [
    "# select the model , need to do testing and quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd517055-1ef9-4450-b532-87017dea5556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please select a model type:\n",
      "0. YOLOx\n",
      "1. RTMDET\n",
      "2. YOLOv7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter choice (0-2):  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please select a RTMDET model variant:\n",
      "0. rtmdet_tiny\n",
      "1. rtmdet_s\n",
      "2. rtmdet_l\n",
      "3. rtmdet_m\n",
      "4. rtmdet_x\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter choice (0-4):  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You selected: rtmdet_s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want to train the model with data augmentation? (0 for no, 1 for yes):  0\n",
      "Do you need to test? (0 for no, 1 for yes):  0\n",
      "Do you want to quantize the model? (0 for no, 1 for yes):  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You selected: rtmdet_s\n",
      "Testing option: no\n",
      "Quantize option: no\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# Dictionary of model types and their corresponding options\n",
    "model_types = {\n",
    "    \"YOLOx\": [\"yolox_tiny\", \"yolox_s\", \"yolox_m\", \"yolox_l\", \"yolox_x\"],\n",
    "    \"RTMDET\": [\"rtmdet_tiny\", \"rtmdet_s\", \"rtmdet_l\", \"rtmdet_m\", \"rtmdet_x\"],\n",
    "    \"YOLOv7\": [\"yolov7_tiny\", \"yolov7_l\", \"yolov7_x\", \"yolov7_w\", \"yolov7_e\"],\n",
    "}\n",
    "\n",
    "# Batch size options\n",
    "batch_size_options = [1, 2, 4, 8, 16]\n",
    "\n",
    "# Variable to store user's choice\n",
    "selected_model_type = None\n",
    "selected_model_variant = None\n",
    "test_option = None\n",
    "path = None\n",
    "quantize_option = None\n",
    "quantize_bits = None\n",
    "batch_size = None\n",
    "AUGMENTATIONS = {\n",
    "    \"noise\": False,\n",
    "    \"denoise\": False,\n",
    "    \"contrast\": False,\n",
    "    \"brightness\": False,\n",
    "    \"rotation\": False\n",
    "}\n",
    "# Print model types\n",
    "print(\"Please select a model type:\")\n",
    "for i, model_type in enumerate(model_types):\n",
    "    print(f\"{i}. {model_type}\")\n",
    "\n",
    "# Get input for model type\n",
    "while True:\n",
    "    model_type_choice = input(\"Enter choice (0-{}): \".format(len(model_types) - 1))\n",
    "    try:\n",
    "        model_type_choice = int(model_type_choice)\n",
    "        if 0 <= model_type_choice < len(model_types):\n",
    "            selected_model_type = list(model_types.keys())[model_type_choice]\n",
    "            break\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "    print(\"Invalid choice, please try again\")\n",
    "\n",
    "# Print model variants for the selected model type\n",
    "print(f\"Please select a {selected_model_type} model variant:\")\n",
    "for i, model_variant in enumerate(model_types[selected_model_type]):\n",
    "    print(f\"{i}. {model_variant}\")\n",
    "\n",
    "# Get input for model variant\n",
    "while True:\n",
    "    model_variant_choice = input(\"Enter choice (0-{}): \".format(len(model_types[selected_model_type]) - 1))\n",
    "    try:\n",
    "        model_variant_choice = int(model_variant_choice)\n",
    "        if 0 <= model_variant_choice < len(model_types[selected_model_type]):\n",
    "            selected_model_variant = model_types[selected_model_type][model_variant_choice]\n",
    "            break\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "    print(\"Invalid choice, please try again\")\n",
    "\n",
    "print(f\"You selected: {selected_model_variant}\")\n",
    "# Ask if the user wants to train the model with data augmentation\n",
    "while True:\n",
    "    train_with_augmentation = input(\"Do you want to train the model with data augmentation? (0 for no, 1 for yes): \")\n",
    "    if train_with_augmentation in [\"0\", \"1\"]:\n",
    "        break\n",
    "    else:\n",
    "        print(\"Invalid choice, please enter '0' or '1'.\")\n",
    "if train_with_augmentation == \"1\":\n",
    "    print(\"Please specify whether you want to enable or disable each augmentation option (0 for False, 1 for True):\")\n",
    "    for augmentation, enabled in AUGMENTATIONS.items():\n",
    "        while True:\n",
    "            choice = input(f\"Do you want to enable {augmentation.capitalize()}? (0 for False, 1 for True): \")\n",
    "            if choice in [\"0\", \"1\"]:\n",
    "                AUGMENTATIONS[augmentation] = choice == \"1\"\n",
    "                break\n",
    "            else:\n",
    "                print(\"Invalid choice, please enter '0' or '1'.\")\n",
    "# Ask if the user wants to test\n",
    "while True:\n",
    "    test_option = input(\"Do you need to test? (0 for no, 1 for yes): \")\n",
    "    if test_option in [\"0\", \"1\"]:\n",
    "        break\n",
    "    else:\n",
    "        print(\"Invalid choice, please enter '0' or '1'.\")\n",
    "\n",
    "# If testing is required, ask for image or video and corresponding path\n",
    "if test_option == \"1\":\n",
    "    while True:\n",
    "        test_type = input(\"Do you want to test on an image or a video? (0 for image, 1 for video): \")\n",
    "        if test_type == \"0\":\n",
    "            path = ''  # You can set a default value or leave it empty for images\n",
    "            break\n",
    "        elif test_type == \"1\":\n",
    "            video_path = input(\"Enter the video path: \")\n",
    "            output_path = input(\"Enter the path to save the video: \")\n",
    "\n",
    "            break\n",
    "        else:\n",
    "            print(\"Invalid choice, please enter '0' or '1'.\")\n",
    "\n",
    "# Now 'path' will be either an empty string for images or the user-provided video path for videos\n",
    "\n",
    "# Ask if the user wants to quantize the model\n",
    "while True:\n",
    "    quantize_option = input(\"Do you want to quantize the model? (0 for no, 1 for yes): \")\n",
    "    if quantize_option in [\"0\", \"1\"]:\n",
    "        break\n",
    "    else:\n",
    "        print(\"Invalid choice, please enter '0' or '1'.\")\n",
    "\n",
    "# If quantizing is required, ask for the quantization bit precision\n",
    "if quantize_option == \"1\":\n",
    "    while True:\n",
    "        quantize_bits = input(\"Choose quantization precision (0 for fp16, 1 for fp32): \")\n",
    "        if quantize_bits in [\"0\", \"1\"]:\n",
    "            if quantize_bits == \"0\" or quantize_bits == \"1\":\n",
    "                while True:\n",
    "                    print(\"Choose batch size:\")\n",
    "                    for i, size in enumerate(batch_size_options):\n",
    "                        print(f\"{i}. {size}\")\n",
    "\n",
    "                    batch_size_choice = input(\"Enter choice (0-{}): \".format(len(batch_size_options) - 1))\n",
    "                    try:\n",
    "                        batch_size_choice = int(batch_size_choice)\n",
    "                        if 0 <= batch_size_choice < len(batch_size_options):\n",
    "                            batch_size = batch_size_options[batch_size_choice]\n",
    "                            break\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "\n",
    "                    print(\"Invalid choice, please try again.\")\n",
    "\n",
    "            break\n",
    "        else:\n",
    "            print(\"Invalid choice, please enter '0' or '1'.\")\n",
    "\n",
    "print(f\"You selected: {selected_model_variant}\")\n",
    "print(f\"Testing option: {'yes' if test_option == '1' else 'no'}\")\n",
    "if test_option == \"1\":\n",
    "    print(f\"Testing on: {'image' if test_type == '0' else 'video'} at path: {path}\")\n",
    "print(f\"Quantize option: {'yes' if quantize_option == '1' else 'no'}\")\n",
    "if quantize_option == \"1\":\n",
    "    print(f\"Quantization precision: {'fp16' if quantize_bits == '0' else 'fp32'}\")\n",
    "    print(f\"Batch size: {batch_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50cd8e46-52c5-4539-9d34-38c1bf7e9465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/viso/sample_dataset/annotations'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "parent_directory = os.path.dirname(annotation_path)\n",
    "parent_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7840c0bc-1517-4232-834e-de96358f2614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'noise': False,\n",
       " 'denoise': False,\n",
       " 'contrast': False,\n",
       " 'brightness': False,\n",
       " 'rotation': False}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AUGMENTATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7baa4af7-f365-4268-b0a7-60ea640520d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/viso/sample_dataset/annotations/total_annotations.json'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotation_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdcacd65-12c7-4374-9bb1-d56bf120f3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "augmented_images\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "def extract_random_middle_part(annotation_file_path, dataset_root):\n",
    "    with open(annotation_file_path, 'r') as f:\n",
    "        annotation_data = json.load(f)\n",
    "\n",
    "    # Get a random image from the list\n",
    "    random_image = random.choice(annotation_data[\"images\"])\n",
    "\n",
    "    file_path = random_image[\"file_name\"]\n",
    "    relative_path = os.path.relpath(file_path, dataset_root)\n",
    "    middle_part = os.path.dirname(relative_path)\n",
    "\n",
    "    return middle_part\n",
    "\n",
    "# Example usage\n",
    "# annotation_file_path = \"/path/to/your/annotation_file.json\"\n",
    "# dataset_root = \"/home/viso/sample_dataset\"\n",
    "\n",
    "random_middle_part = extract_random_middle_part(annotation_path, dataset_root)\n",
    "\n",
    "# Print the extracted middle part\n",
    "print(random_middle_part)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125dd5c8-4717-4850-85da-5862e39a230c",
   "metadata": {},
   "source": [
    "# Generating metainfo for the config based on the annotation file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49a1b83e-f5ac-4274-a390-315bc8ba32be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Names: ['dumper_loaded', 'dumper_empty', 'wheel_loader', 'excavator']\n",
      "Number of Classes: 4\n",
      "Metainfo: {'classes': ['dumper_loaded', 'dumper_empty', 'wheel_loader', 'excavator'], 'palette': [(0, 0, 0), (50, 100, 150), (100, 200, 45), (150, 45, 195)]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(annotation_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "classes = [c['name'] for c in data['categories']]\n",
    "NUM_CLASSES = len(classes)\n",
    "palette = []\n",
    "\n",
    "for i, c in enumerate(classes):\n",
    "    r = (i * 50) % 255\n",
    "    g = (i * 100) % 255\n",
    "    b = (i * 150) % 255\n",
    "    palette.append((r, g, b))\n",
    "\n",
    "metainfo = {\n",
    "    'classes': classes,\n",
    "    'palette': palette\n",
    "}\n",
    "class_names=metainfo['classes']\n",
    "print(\"Class Names:\", class_names)\n",
    "print(\"Number of Classes:\", NUM_CLASSES)\n",
    "print(\"Metainfo:\", metainfo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28c1afd-9791-4639-b3f5-f250555d9144",
   "metadata": {},
   "source": [
    "# Data Augmentation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2057d901-bc51-408c-b982-83f9121b146a",
   "metadata": {},
   "source": [
    "# converting the annotations from json to txt  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81e2db8f-c315-412d-b9f8-cf201eca6a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import shutil\n",
    "# Load COCO annotations \n",
    "def convert_coco_to_yolo_old(annotation_path,dataset_root):\n",
    "    print(\".......... in convert_coco_to_yolo ..........\")\n",
    "    print(annotation_path)\n",
    "    # Ensure the \"txt_files\" directory exists or create it\n",
    "    txt_files_dir = os.path.join(dataset_root, 'txt_files')\n",
    "    print(txt_files_dir)\n",
    "    if not os.path.exists(txt_files_dir):\n",
    "        os.makedirs(txt_files_dir)\n",
    "\n",
    "    with open(annotation_path) as f:\n",
    "        coco = json.load(f)\n",
    "\n",
    "\n",
    "    # Create lists to store image info and annotations\n",
    "    image_ids = []\n",
    "    image_info = []\n",
    "    annotations = []\n",
    "    print(\"image_ids\",image_ids)\n",
    "    print(\"image_info\",image_info)\n",
    "    print(\"annotations\",annotations)\n",
    "    # Get image ids and image info \n",
    "    for img in coco['images']:\n",
    "        image_ids.append(img['id'])\n",
    "        image_info.append({'file_name': img['file_name'],\n",
    "                           'width': img['width'],\n",
    "                           'height': img['height']})\n",
    "        # print(\"image id are\",image_ids)\n",
    "    # print(\".........................\")\n",
    "    # Get annotations \n",
    "    for ann in coco['annotations']:\n",
    "        bbox = ann['bbox']\n",
    "        x, y, width, height = bbox\n",
    "        image_id = ann['image_id']\n",
    "        category_id = ann['category_id']\n",
    "        # print(\"image id 'ssssssssss\",image_id)\n",
    "        # Convert to relative cooridnates \n",
    "        x_rel = x/image_info[image_id-1]['width'] \n",
    "        y_rel = y/image_info[image_id-1]['height']\n",
    "        width_rel = width/image_info[image_id-1]['width']\n",
    "        height_rel = height/image_info[image_id-1]['height']\n",
    "\n",
    "        annotation = (category_id, x_rel, y_rel, width_rel, height_rel)\n",
    "        annotations.append({'image_id': image_id, 'annotation': annotation})\n",
    "\n",
    "    # Convert to YOLO format\n",
    "    yolo_annotations = []\n",
    "    for annotation in annotations:\n",
    "        image_id = annotation['image_id']\n",
    "        # print(\"image_id\",image_id)\n",
    "        category_id, x, y, width, height = annotation['annotation']\n",
    "\n",
    "        # Calculate center x, y \n",
    "        x_center = x + width/2\n",
    "        y_center = y + height/2\n",
    "\n",
    "        yolo_annotation = f\"{category_id} {x_center} {y_center} {width} {height}\\n\"\n",
    "        yolo_annotations.append({'image_id': image_id, 'annotation': yolo_annotation})\n",
    "\n",
    "    # Group annotations by image    \n",
    "    yolo_files = {}\n",
    "    for annotation in yolo_annotations:\n",
    "        print(yolo_annotations)\n",
    "        image_id = annotation['image_id']\n",
    "        yolo_annotation = annotation['annotation']\n",
    "        if image_id not in yolo_files:\n",
    "            yolo_files[image_id] = [yolo_annotation]\n",
    "        else:\n",
    "            yolo_files[image_id].append(yolo_annotation)\n",
    "\n",
    "\n",
    "    # Save annotation files\n",
    "        for image_id, annotations in yolo_files.items(): \n",
    "            file_name = image_info[image_id-1]['file_name']\n",
    "            # print('.1.',file_name)\n",
    "            file_name = file_name.split('.')[0] + '.txt'\n",
    "            # print('.2.',file_name)\n",
    "            base_name = os.path.splitext(os.path.basename(file_name))[0]  # Get the base name without extension\n",
    "\n",
    "            # Construct the destination .txt file name\n",
    "            txt_file_name = base_name + '.txt'\n",
    "            file_path = os.path.join(txt_files_dir, txt_file_name)\n",
    "\n",
    "            #print(f\"Saving to: {file_path}\")\n",
    "\n",
    "            with open(file_path, 'w') as f:\n",
    "            # with open(txt_files_dir, 'w') as f:\n",
    "                for annotation in annotations:\n",
    "                    f.write(annotation)\n",
    "\n",
    "            # with open(file_path, 'w') as f:\n",
    "            #     for annotation in annotations:\n",
    "            #         f.write(annotation + '\\n')\n",
    "            #         # destination_path = os.path.join(txt_files_dir, file_name)\n",
    "            #         shutil.move(file_path, txt_files_dir)\n",
    "\n",
    "\n",
    "\n",
    "    # Save image list file\n",
    "    # with open('train.txt', 'w') as f:\n",
    "    #     for image_id in image_ids:\n",
    "    #         file_name = image_info[image_id-1]['file_name']\n",
    "    #         f.write(file_name + '\\n')\n",
    "    \n",
    "#convert_coco_to_yolo(annotation_path, dataset_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ced0d4e-d85e-4576-abaa-10428cc40e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def convert_coco_to_yolo(annotation_path, dataset_root):\n",
    "    print(\".......... in convert_coco_to_yolo ..........\")\n",
    "    print(annotation_path)\n",
    "\n",
    "    # Ensure the \"txt_files\" directory exists or create it\n",
    "    txt_files_dir = os.path.join(dataset_root, 'txt_files')\n",
    "    print(txt_files_dir)\n",
    "    if not os.path.exists(txt_files_dir):\n",
    "        os.makedirs(txt_files_dir)\n",
    "\n",
    "    with open(annotation_path) as f:\n",
    "        coco = json.load(f)\n",
    "\n",
    "    # Create lists to store image info and annotations\n",
    "    image_info = {}\n",
    "    annotations = []\n",
    "\n",
    "    # Get image info\n",
    "    for img in coco['images']:\n",
    "        image_info[img['id']] = {'file_name': img['file_name'],\n",
    "                                  'width': img['width'],\n",
    "                                  'height': img['height']}\n",
    "\n",
    "    # Get annotations \n",
    "    for ann in coco['annotations']:\n",
    "        bbox = ann['bbox']\n",
    "        x, y, width, height = bbox\n",
    "        image_id = ann['image_id']\n",
    "        if image_id not in image_info:\n",
    "            print(f\"Warning: Image ID {image_id} not found in image info\")\n",
    "            continue\n",
    "\n",
    "        category_id = ann['category_id']\n",
    "        \n",
    "        # Convert to relative coordinates \n",
    "        x_rel = x / image_info[image_id]['width']\n",
    "        y_rel = y / image_info[image_id]['height']\n",
    "        width_rel = width / image_info[image_id]['width']\n",
    "        height_rel = height / image_info[image_id]['height']\n",
    "\n",
    "        annotation = (category_id, x_rel, y_rel, width_rel, height_rel)\n",
    "        annotations.append({'image_id': image_id, 'annotation': annotation})\n",
    "\n",
    "    # Convert to YOLO format\n",
    "    yolo_annotations = []\n",
    "    for annotation in annotations:\n",
    "        image_id = annotation['image_id']\n",
    "        category_id, x, y, width, height = annotation['annotation']\n",
    "\n",
    "        # Calculate center x, y \n",
    "        x_center = x + width / 2\n",
    "        y_center = y + height / 2\n",
    "\n",
    "        yolo_annotation = f\"{category_id} {x_center} {y_center} {width} {height}\\n\"\n",
    "        yolo_annotations.append({'image_id': image_id, 'annotation': yolo_annotation})\n",
    "\n",
    "    # Group annotations by image    \n",
    "    yolo_files = {}\n",
    "    for annotation in yolo_annotations:\n",
    "        image_id = annotation['image_id']\n",
    "        yolo_annotation = annotation['annotation']\n",
    "        if image_id not in yolo_files:\n",
    "            yolo_files[image_id] = [yolo_annotation]\n",
    "        else:\n",
    "            yolo_files[image_id].append(yolo_annotation)\n",
    "\n",
    "    # Save annotation files\n",
    "    for image_id, annotations in yolo_files.items(): \n",
    "        if image_id not in image_info:\n",
    "            print(f\"Error: Image ID {image_id} not found in image info\")\n",
    "            continue\n",
    "            \n",
    "        file_name = image_info[image_id]['file_name']\n",
    "        base_name = os.path.splitext(os.path.basename(file_name))[0]  # Get the base name without extension\n",
    "        txt_file_name = base_name + '.txt'\n",
    "        file_path = os.path.join(txt_files_dir, txt_file_name)\n",
    "\n",
    "        with open(file_path, 'w') as f:\n",
    "            for annotation in annotations:\n",
    "                f.write(annotation)\n",
    "\n",
    "    # Save image list file\n",
    "    # with open('train.txt', 'w') as f:\n",
    "    #     for image_id in image_ids:\n",
    "    #         file_name = image_info[image_id-1]['file_name']\n",
    "    #         f.write(file_name + '\\n')\n",
    "\n",
    "# Now let's call the function\n",
    "# annotation_path = \"/home/viso/sample_data/annotations/total_annotations.json\"\n",
    "# dataset_root = \"/home/viso/sample_data/augmented_images\"\n",
    "# convert_coco_to_yolo(annotation_path, dataset_root)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe0b864-c976-4340-aa47-58a5e33d3a72",
   "metadata": {},
   "source": [
    "# creating the config file for the data augmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16606162-77b5-4703-8848-403194b84f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_and_save_config(random_middle_part, dataset_root,AUGMENTATIONS):\n",
    "    print(\"............edit_and_save_config..............\")\n",
    "    %cd /home/viso/datasets/enhance_script\n",
    "    import json\n",
    "    new_config_path = '/home/viso/datasets/enhance_script/dataset_enhance_config.json'\n",
    "    original_config_path = '/home/viso/datasets/enhance_script/dataset_enhance_config-dummy.json'\n",
    "    # Load the original config\n",
    "    with open(original_config_path) as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    # Modify the content as needed\n",
    "    config[\"original_image_folder\"] = f'{dataset_root}/{random_middle_part}'\n",
    "    config[\"original_label_folder\"] = f'{dataset_root}/txt_files'\n",
    "    config[\"enhance_image_folder\"] = f'{dataset_root}/train'\n",
    "    config[\"enhance_label_folder\"] = f'{dataset_root}/train'\n",
    "    config[\"augmentations\"] = {\n",
    "        \"noise\": AUGMENTATIONS['noise'],\n",
    "        \"denoise\": AUGMENTATIONS['denoise'],\n",
    "        \"contrast\": AUGMENTATIONS['contrast'],\n",
    "        \"brightness\": AUGMENTATIONS['brightness'],\n",
    "        \"rotation\": AUGMENTATIONS['rotation']\n",
    "    }\n",
    "    # Save the modified config to a new file\n",
    "    with open(new_config_path, 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "# random_middle_part = 'images'\n",
    "# dataset_root = '/home/viso/sample_dataset'  # Provide the actual path\n",
    "# edit_and_save_config(original_config_path, new_config_path, random_middle_part, dataset_root)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ba6bb0-02f1-45f7-9263-cdeb04c10893",
   "metadata": {},
   "source": [
    "# Getting all the image_paths in the txt file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbb9a9a2-48c3-4777-a1b1-672c853c256c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_image_paths(folder_path, extensions=[\".jpg\", \".png\"]):\n",
    "    image_paths = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if any(file.lower().endswith(ext) for ext in extensions):\n",
    "                image_paths.append(os.path.join(root, file))\n",
    "    return image_paths\n",
    "\n",
    "def write_paths_to_file(image_paths, output_file=f\"{dataset_root}/train.txt\"):\n",
    "    with open(output_file, \"w\") as file:\n",
    "        for path in image_paths:\n",
    "            file.write(path + \"\\n\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "# folder_path = \"/home/viso/sample_data/augmented_imagges\"  # Replace with the actual path to your folder\n",
    "    # image_paths = get_image_paths(folder_path)\n",
    "    # write_paths_to_file(image_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dcf5beb9-8b91-4941-9bbd-fa2751095a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def update_config_with_class_names(class_names):\n",
    "    dummy_main = '/home/viso/github/Yolo-to-COCO-format-converter/dummy_main.py'\n",
    "    main='/home/viso/github/Yolo-to-COCO-format-converter/main.py'\n",
    "    with open(dummy_main, 'r') as f:\n",
    "        cfg_content = f.read()\n",
    "\n",
    "    # Replace specific lines with new values\n",
    "    cfg_content = cfg_content.replace(\"classes =\", f\"classes = {class_names}\")\n",
    "\n",
    "    # Write the updated content back to the configuration file\n",
    "    with open(main, 'w') as custom_cfg_file:\n",
    "        custom_cfg_file.write(cfg_content)\n",
    "\n",
    "# Example usage:\n",
    "# number_of_classes = 10  # Replace with the actual number of classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fdd78478-fdca-41c9-990e-a1d3c8c5ef5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "if train_with_augmentation=='1':\n",
    "    # %cd /home/viso/notebooks\n",
    "    # %run ./aug.ipynb\n",
    "    convert_coco_to_yolo(annotation_path, dataset_root)\n",
    "    edit_and_save_config(random_middle_part, dataset_root,AUGMENTATIONS)\n",
    "    %cd /home/viso/datasets/enhance_script\n",
    "    !python run_dataset_enhance.py\n",
    "    %cd /home/viso/github/Yolo-to-COCO-format-converter\n",
    "    update_config_with_class_names(class_names)\n",
    "    !python main.py --path {dataset_root} --output output_json.json\n",
    "    shutil.copy('/home/viso/github/Yolo-to-COCO-format-converter/output/output_json.json',parent_directory)\n",
    "    %cd {dataset_root}\n",
    "    folder_path=f'{dataset_root}/train'\n",
    "    image_paths = get_image_paths(folder_path)\n",
    "    write_paths_to_file(image_paths)\n",
    "    with open('object.names', 'w') as f:\n",
    "        for class_name in class_names:\n",
    "            f.write(class_name + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "941c9457-ac5f-47e6-b665-b3e671aabab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_with_augmentation=='1':\n",
    "    annotation_path = f'{parent_directory}/output_json.json'\n",
    "    print(annotation_path)\n",
    "    random_middle_part = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7381d75-7922-4a96-9638-8d63c026b72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(type(AUGMENTATIONS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0210a761-5947-4ba5-bbf4-4f2236e69e1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AUGMENTATIONS['noise']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6649f31-69dd-4902-b18d-af266a872d43",
   "metadata": {},
   "source": [
    "# split the annotation file into train , test and validation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5add7c-530d-4662-9ea9-923c50fda5c6",
   "metadata": {},
   "source": [
    "### If there is single annotation file , that can divide into train , test and validation (coco format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f6205f4-3f95-49b7-85fb-d5b2038f9949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/viso/github/cocosplit_train_test_valid\n"
     ]
    }
   ],
   "source": [
    "cd /home/viso/github/cocosplit_train_test_valid/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "360c77d1-6b0e-479c-87d8-6ffe7bf07d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sklearn in /home/viso/.local/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (0.0.post11)\n",
      "Requirement already satisfied: funcy in /home/viso/.local/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (2.0)\n",
      "Collecting argparse (from -r requirements.txt (line 3))\n",
      "  Using cached argparse-1.4.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: scikit-learn in /home/viso/.local/lib/python3.8/site-packages (from -r requirements.txt (line 4)) (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /home/viso/.local/lib/python3.8/site-packages (from scikit-learn->-r requirements.txt (line 4)) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /home/viso/.local/lib/python3.8/site-packages (from scikit-learn->-r requirements.txt (line 4)) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/viso/.local/lib/python3.8/site-packages (from scikit-learn->-r requirements.txt (line 4)) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/viso/.local/lib/python3.8/site-packages (from scikit-learn->-r requirements.txt (line 4)) (3.2.0)\n",
      "Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
      "Installing collected packages: argparse\n",
      "Successfully installed argparse-1.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "abf50abe-116d-476b-bb6b-e8a98532582b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: cocosplit_train_test_valid.py [-h] [--train_ratio RATIO_TRAIN]\n",
      "                                     [--valid_ratio RATIO_VALID]\n",
      "                                     [--test_ratio RATIO_TEST]\n",
      "                                     [--trainJson_name TRAINJSON_NAME]\n",
      "                                     [--validJson_name VALIDJSON_NAME]\n",
      "                                     [--testJson_name TESTJSON_NAME]\n",
      "                                     [--annotations]\n",
      "                                     coco_annotations\n",
      "cocosplit_train_test_valid.py: error: argument --valid_ratio: invalid float value: '0.1.'\n"
     ]
    }
   ],
   "source": [
    "!python cocosplit_train_test_valid.py \\\n",
    "    --annotations \"{annotation_path}\" \\\n",
    "    --train_ratio {train_percentage} \\\n",
    "    --valid_ratio {val_percentage} \\\n",
    "    --test_ratio {test_percentage} \\\n",
    "    --trainJson_name train.json \\\n",
    "    --validJson_name val.json \\\n",
    "    --testJson_name test.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "379527e5-90ea-43fa-90d3-9a7ee8705c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotations/train.json annotations/val.json annotations/test.json\n",
      "/home/viso/sample_dataset/annotations\n"
     ]
    }
   ],
   "source": [
    "import os , shutil\n",
    "# dataset_root = '/home/viso/datasets/viso_datasets/construction-demo'\n",
    "\n",
    "# parent_directory = os.path.dirname(annotation_path)\n",
    "shutil.copy('/home/viso/github/cocosplit_train_test_valid/test.json',parent_directory)\n",
    "shutil.copy('/home/viso/github/cocosplit_train_test_valid/val.json',parent_directory)\n",
    "shutil.copy('/home/viso/github/cocosplit_train_test_valid/train.json',parent_directory)\n",
    "\n",
    "train_json = os.path.join(parent_directory, 'train.json')\n",
    "val_json = os.path.join(parent_directory, 'val.json')\n",
    "test_json = os.path.join(parent_directory, 'test.json')\n",
    "\n",
    "# Remove the dataset_root prefix using os.path.relpath\n",
    "train_json_relpath = os.path.relpath(train_json, dataset_root)\n",
    "val_json_relpath = os.path.relpath(val_json, dataset_root)\n",
    "test_json_relpath = os.path.relpath(test_json, dataset_root)\n",
    "\n",
    "print(train_json_relpath, val_json_relpath, test_json_relpath)\n",
    "print(parent_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d964eb5-bb9f-4b6c-8307-3194f9d7ac3f",
   "metadata": {},
   "source": [
    "# Yolox configuration setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f0bc5ce8-a5fa-40f3-9bb5-cb7a6e30ca1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the selected model type is 'YOLOx'\n",
    "if selected_model_type == 'YOLOx':\n",
    "    # Provide the path to the user for modification\n",
    "    dummy_config = '/home/viso/mmyolo/configs/yolox/custom_config.py'\n",
    "    custom_config_file_path_base = '/home/viso/mmyolo/configs/yolox/yolox_s_fast_8xb8-300e_coco.py'  # Set your desired path\n",
    "\n",
    "    # Load the YOLOx config file\n",
    "    with open(dummy_config, 'r') as f:\n",
    "        cfg_content = f.read()\n",
    "\n",
    "    # Replace specific lines with new values\n",
    "    cfg_content = cfg_content.replace(\"data_root =\", f\"data_root = '{dataset_root}/'\")\n",
    "    cfg_content = cfg_content.replace(\"train_data_prefix =\", f\"train_data_prefix = '{random_middle_part}'\")\n",
    "    cfg_content = cfg_content.replace(\"val_data_prefix =\", f\"val_data_prefix = '{random_middle_part}'\")\n",
    "    cfg_content = cfg_content.replace(\"train_ann_file =\", f\"train_ann_file = '{train_json_relpath}'\")\n",
    "    cfg_content = cfg_content.replace(\"val_ann_file =\", f\"val_ann_file = '{val_json_relpath}'\")\n",
    "    cfg_content = cfg_content.replace(\"num_classes =\", f\"num_classes = {num_classes}\")\n",
    "    cfg_content = cfg_content.replace(\"meta_info =\", f\"meta_info = {metainfo}\")\n",
    "    cfg_content = cfg_content.replace(\"max_epochs =\", f\"max_epochs = {MAX_EPOCHS}\")\n",
    "    cfg_content = cfg_content.replace(\"train_batch_size_per_gpu =\", f\"train_batch_size_per_gpu = {BATCH_SIZE}\")\n",
    "\n",
    "    # Save the modified content to the new file\n",
    "    with open(custom_config_file_path_base, 'w') as custom_cfg_file:\n",
    "        custom_cfg_file.write(cfg_content)\n",
    "\n",
    "    print(f\"Custom config file has been created at {custom_config_file_path_base}.\")\n",
    "    print(\"dummy_config:\", dummy_config)\n",
    "    print(\"custom_config_file_path_base:\", custom_config_file_path_base)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bed28c-b4cb-4a35-b46d-81c339d32b88",
   "metadata": {},
   "source": [
    "# Yolov7 configuration setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "007e4441-bbff-466d-90ce-03bc6847797b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if selected_model_type == 'YOLOv7':\n",
    "    dummy_config = '/home/viso/mmyolo/configs/yolov7/custom_config.py'\n",
    "    custom_config_file_path_base = '/home/viso/mmyolo/configs/yolov7/yolov7_l_syncbn_fast_8x16b-300e_coco.py'  # Set your desired path\n",
    "\n",
    "    # Load the YOLOx config file\n",
    "    with open(dummy_config, 'r') as f:\n",
    "        cfg_content = f.read()\n",
    "\n",
    "    # Replace specific lines with new values\n",
    "    cfg_content = cfg_content.replace(\"data_root =\", f\"data_root = '{dataset_root}/'\")\n",
    "    cfg_content = cfg_content.replace(\"train_data_prefix =\", f\"train_data_prefix = '{random_middle_part}'\")\n",
    "    cfg_content = cfg_content.replace(\"val_data_prefix =\", f\"val_data_prefix = '{random_middle_part}'\")\n",
    "    cfg_content = cfg_content.replace(\"train_ann_file =\", f\"train_ann_file = '{train_json_relpath}'\")\n",
    "    cfg_content = cfg_content.replace(\"val_ann_file =\", f\"val_ann_file = '{val_json_relpath}'\")\n",
    "    cfg_content = cfg_content.replace(\"num_classes =\", f\"num_classes = {NUM_CLASSES}\")\n",
    "    cfg_content = cfg_content.replace(\"meta_info =\", f\"meta_info = {metainfo}\")\n",
    "    cfg_content = cfg_content.replace(\"max_epochs =\", f\"max_epochs = {MAX_EPOCHS}\")\n",
    "    cfg_content = cfg_content.replace(\"train_batch_size_per_gpu =\", f\"train_batch_size_per_gpu = {BATCH_SIZE}\")\n",
    "\n",
    "    # Save the modified content to the new file\n",
    "    with open(custom_config_file_path_base, 'w') as custom_cfg_file:\n",
    "        custom_cfg_file.write(cfg_content)\n",
    "\n",
    "    print(f\"Custom config file has been created at {custom_config_file_path_base}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc01c3fc-88b1-4ec2-880e-ea31adf4284f",
   "metadata": {},
   "source": [
    "# Rtmdet configuration setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "981330f5-0a26-4c0e-813e-ee4cef295fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom config file has been created at /home/viso/mmyolo/configs/rtmdet/rtmdet_l_syncbn_fast_8xb32-300e_coco.py.\n",
      "dummy_config: /home/viso/mmyolo/configs/rtmdet/custom_config.py\n"
     ]
    }
   ],
   "source": [
    "if selected_model_type == 'RTMDET':\n",
    "    dummy_config = '/home/viso/mmyolo/configs/rtmdet/custom_config.py'\n",
    "    custom_config_file_path_base = '/home/viso/mmyolo/configs/rtmdet/rtmdet_l_syncbn_fast_8xb32-300e_coco.py'  # Set your desired path\n",
    "\n",
    "    # Load the YOLOx config file\n",
    "    with open(dummy_config, 'r') as f:\n",
    "        cfg_content = f.read()\n",
    "\n",
    "    # Replace specific lines with new values\n",
    "    cfg_content = cfg_content.replace(\"data_root =\", f\"data_root = '{dataset_root}/'\")\n",
    "    cfg_content = cfg_content.replace(\"train_data_prefix =\", f\"train_data_prefix = '{random_middle_part}'\")\n",
    "    cfg_content = cfg_content.replace(\"val_data_prefix =\", f\"val_data_prefix = '{random_middle_part}'\")\n",
    "    cfg_content = cfg_content.replace(\"train_ann_file =\", f\"train_ann_file = '{train_json_relpath}'\")\n",
    "    cfg_content = cfg_content.replace(\"val_ann_file =\", f\"val_ann_file = '{val_json_relpath}'\")\n",
    "    cfg_content = cfg_content.replace(\"num_classes =\", f\"num_classes = {NUM_CLASSES}\")\n",
    "    cfg_content = cfg_content.replace(\"meta_info =\", f\"meta_info = {metainfo}\")\n",
    "    cfg_content = cfg_content.replace(\"max_epochs =\", f\"max_epochs = {MAX_EPOCHS}\")\n",
    "    cfg_content = cfg_content.replace(\"train_batch_size_per_gpu =\", f\"train_batch_size_per_gpu = {BATCH_SIZE}\")\n",
    "\n",
    "    # Save the modified content to the new file\n",
    "    with open(custom_config_file_path_base, 'w') as custom_cfg_file:\n",
    "        custom_cfg_file.write(cfg_content)\n",
    "\n",
    "    print(f\"Custom config file has been created at {custom_config_file_path_base}.\")\n",
    "    print(\"dummy_config:\", dummy_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1cc427b9-d53a-46ca-b389-b560cdb76d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "if selected_model_variant=='yolox_tiny':\n",
    "    config_file_path = '/home/viso/mmyolo/configs/yolox/yolox_tiny_fast_8xb8-300e_coco.py'\n",
    "elif selected_model_variant == 'yolox_s':\n",
    "    config_file_path = '/home/viso/mmyolo/configs/yolox/yolox_s_fast_8xb8-300e_coco.py'\n",
    "elif selected_model_variant == 'yolox_m':\n",
    "    config_file_path = '/home/viso/mmyolo/configs/yolox/yolox_m_fast_8xb8-300e_coco.py'\n",
    "elif selected_model_variant == 'yolox_l':\n",
    "    config_file_path = '/home/viso/mmyolo/configs/yolox/yolox_l_fast_8xb8-300e_coco.py'\n",
    "elif selected_model_variant == 'yolox_x':\n",
    "    config_file_path = '/home/viso/mmyolo/configs/yolox/yolox_x_fast_8xb8-300e_coco.py'\n",
    "elif selected_model_variant == 'yolov7_tiny':\n",
    "    config_file_path = '/home/viso/mmyolo/configs/yolov7/yolov7_tiny_syncbn_fast_8x16b-300e_coco.py'\n",
    "elif selected_model_variant == 'yolov7_l':\n",
    "    config_file_path = '/home/viso/mmyolo/configs/yolov7/yolov7_l_syncbn_fast_8x16b-300e_coco.py'\n",
    "elif selected_model_variant == 'yolov7_x':\n",
    "    config_file_path = '/home/viso/mmyolo/configs/yolov7/yolov7_x_syncbn_fast_8x16b-300e_coco.py'\n",
    "elif selected_model_variant == 'yolov7_w':\n",
    "    config_file_path = '/home/viso/mmyolo/configs/yolov7/yolov7_w-p6_syncbn_fast_8x16b-300e_coco.py'\n",
    "elif selected_model_variant == 'yolov7_e':\n",
    "    config_file_path = '/home/viso/mmyolo/configs/yolov7/yolov7_e_syncbn_fast_8x16b-300e_coco.py'\n",
    "elif selected_model_variant == 'rtmdet_tiny':\n",
    "    config_file_path = '/home/viso/mmyolo/configs/rtmdet/rtmdet_tiny_syncbn_fast_8xb32-300e_coco.py'\n",
    "elif selected_model_variant == 'rtmdet_s':\n",
    "    config_file_path = '/home/viso/mmyolo/configs/rtmdet/rtmdet_s_syncbn_fast_8xb32-300e_coco.py'\n",
    "elif selected_model_variant == 'rtmdet_l':\n",
    "    config_file_path = '/home/viso/mmyolo/configs/rtmdet/rtmdet_l_syncbn_fast_8xb32-300e_coco.py'\n",
    "elif selected_model_variant == 'rtmdet_m':\n",
    "    config_file_path = '/home/viso/mmyolo/configs/rtmdet/rtmdet_m_syncbn_fast_8xb32-300e_coco.py'\n",
    "elif selected_model_variant == 'rtmdet_x':\n",
    "    config_file_path = '/home/viso/mmyolo/configs/rtmdet/rtmdet_x_syncbn_fast_8xb32-300e_coco.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "169e7c1b-0e54-408e-a820-b0ce815eee55",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_config_file_path=config_file_path.split('/')[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983be65b-97e5-4a1b-b71f-5645356b78f0",
   "metadata": {},
   "source": [
    "# train the data with the selected model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7f15b54d-a7fa-4e47-8077-baae4ecd0cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/viso/mmyolo\n"
     ]
    }
   ],
   "source": [
    "%cd /home/viso/mmyolo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bce68745-7330-4f09-80c3-b21b808b3d8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rtmdet'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_model_type=selected_model_type.lower()\n",
    "selected_model_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "929aae35-73cf-4840-8c15-6dd51541cbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q setuptools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c75006f9-7aa4-4e13-b0d2-0eea214c6636",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"tools/train.py\", line 13, in <module>\n",
      "    from mmyolo.utils import is_metainfo_lower\n",
      "  File \"/home/viso/github/mmyolo/mmyolo/utils/__init__.py\", line 3, in <module>\n",
      "    from .misc import is_metainfo_lower, switch_to_deploy\n",
      "  File \"/home/viso/github/mmyolo/mmyolo/utils/misc.py\", line 10, in <module>\n",
      "    from mmyolo.models import RepVGGBlock\n",
      "  File \"/home/viso/github/mmyolo/mmyolo/models/__init__.py\", line 2, in <module>\n",
      "    from .backbones import *  # noqa: F401,F403\n",
      "  File \"/home/viso/github/mmyolo/mmyolo/models/backbones/__init__.py\", line 3, in <module>\n",
      "    from .csp_darknet import YOLOv5CSPDarknet, YOLOv8CSPDarknet, YOLOXCSPDarknet\n",
      "  File \"/home/viso/github/mmyolo/mmyolo/models/backbones/csp_darknet.py\", line 7, in <module>\n",
      "    from mmdet.models.backbones.csp_darknet import CSPLayer, Focus\n",
      "  File \"/home/viso/github/mmdetection/mmdet/models/__init__.py\", line 2, in <module>\n",
      "    from .backbones import *  # noqa: F401,F403\n",
      "  File \"/home/viso/github/mmdetection/mmdet/models/backbones/__init__.py\", line 2, in <module>\n",
      "    from .csp_darknet import CSPDarknet\n",
      "  File \"/home/viso/github/mmdetection/mmdet/models/backbones/csp_darknet.py\", line 11, in <module>\n",
      "    from ..layers import CSPLayer\n",
      "  File \"/home/viso/github/mmdetection/mmdet/models/layers/__init__.py\", line 3, in <module>\n",
      "    from .bbox_nms import fast_nms, multiclass_nms\n",
      "  File \"/home/viso/github/mmdetection/mmdet/models/layers/bbox_nms.py\", line 5, in <module>\n",
      "    from mmcv.ops.nms import batched_nms\n",
      "  File \"/home/viso/.local/lib/python3.8/site-packages/mmcv/ops/__init__.py\", line 2, in <module>\n",
      "    from .active_rotated_filter import active_rotated_filter\n",
      "  File \"/home/viso/.local/lib/python3.8/site-packages/mmcv/ops/active_rotated_filter.py\", line 10, in <module>\n",
      "    ext_module = ext_loader.load_ext(\n",
      "  File \"/home/viso/.local/lib/python3.8/site-packages/mmcv/utils/ext_loader.py\", line 13, in load_ext\n",
      "    ext = importlib.import_module('mmcv.' + name)\n",
      "  File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "ImportError: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error executing the command. Return code: 1\n",
      "Output: None\n",
      "Error: None\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Assuming config_file_path is something like \"yolox/config_file.py\"\n",
    "# config_file_path = config_file_path.split('/')[-1]\n",
    "command_line = f\"python tools/train.py configs/{selected_model_type}/{custom_config_file_path}\"\n",
    "\n",
    "# Run the command\n",
    "result = subprocess.run(command_line, shell=True)\n",
    "\n",
    "# Check the result\n",
    "if result.returncode == 0:\n",
    "    print(\"Command executed successfully\")\n",
    "else:\n",
    "    print(f\"Error executing the command. Return code: {result.returncode}\")\n",
    "    print(f\"Output: {result.stdout}\")\n",
    "    print(f\"Error: {result.stderr}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2adb0c21-57b6-4b38-b80a-7454627dc4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: mmcv-full==${1.3.15}: bad substitution\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade mmcv-full==${1.3.15} -f https://download.openmmlab.com/mmcv/dist/cu${CUDA}/torch${PYTORCH}/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ebcb1af8-195b-4308-a96f-ea1e1e0876d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torch==2.0.1\n",
      "  Downloading torch-2.0.1-cp38-cp38-manylinux1_x86_64.whl.metadata (24 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from torch==2.0.1) (3.4.2)\n",
      "Requirement already satisfied: typing-extensions in /home/viso/.local/lib/python3.8/site-packages (from torch==2.0.1) (4.9.0)\n",
      "Requirement already satisfied: sympy in /home/viso/.local/lib/python3.8/site-packages (from torch==2.0.1) (1.12)\n",
      "Requirement already satisfied: networkx in /home/viso/.local/lib/python3.8/site-packages (from torch==2.0.1) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.8/site-packages (from torch==2.0.1) (3.1.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.1)\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.1)\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.1)\n",
      "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.1)\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.1)\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.1)\n",
      "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.1)\n",
      "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.1)\n",
      "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.1)\n",
      "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.1)\n",
      "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.1)\n",
      "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==2.0.0 (from torch==2.0.1)\n",
      "  Downloading triton-2.0.0-1-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: setuptools in /home/viso/.local/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (59.5.0)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (0.37.1)\n",
      "Collecting cmake (from triton==2.0.0->torch==2.0.1)\n",
      "  Downloading cmake-3.29.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting lit (from triton==2.0.0->torch==2.0.1)\n",
      "  Downloading lit-18.1.3-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/viso/.local/lib/python3.8/site-packages (from jinja2->torch==2.0.1) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/viso/.local/lib/python3.8/site-packages (from sympy->torch==2.0.1) (1.3.0)\n",
      "Downloading torch-2.0.1-cp38-cp38-manylinux1_x86_64.whl (619.9 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading triton-2.0.0-1-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.2 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m63.2/63.2 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading cmake-3.29.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading lit-18.1.3-py3-none-any.whl (96 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lit, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, cmake, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch\n",
      "\u001b[33m  WARNING: The script lit is installed in '/home/viso/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The scripts cmake, cpack and ctest are installed in '/home/viso/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: triton\n",
      "    Found existing installation: triton 2.2.0\n",
      "    Uninstalling triton-2.2.0:\n",
      "      Successfully uninstalled triton-2.2.0\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.9.0\n",
      "    Uninstalling torch-1.9.0:\n",
      "      Successfully uninstalled torch-1.9.0\n",
      "\u001b[33m  WARNING: The scripts convert-caffe2-to-onnx, convert-onnx-to-caffe2 and torchrun are installed in '/home/viso/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 0.7.2 requires torch==1.7.1, but you have torch 2.0.1 which is incompatible.\n",
      "torchvision 0.17.1 requires torch==2.2.1, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed cmake-3.29.2 lit-18.1.3 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.1 triton-2.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==2.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e8d671d9-4c40-4f51-8b2c-aad9ae1d5280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: mmcv 1.3.15\n",
      "Uninstalling mmcv-1.3.15:\n",
      "  Successfully uninstalled mmcv-1.3.15\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y mmcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "152de48f-8fa2-4de0-82d9-23ed728f6685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.2.1\n",
      "Uninstalling torch-2.2.1:\n",
      "  Successfully uninstalled torch-2.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7bd8d1a9-4303-4b25-8dbb-0593aeadea20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.8/site-packages (1.11.0)\n",
      "Requirement already satisfied: typing_extensions in /home/viso/.local/lib/python3.8/site-packages (from torch) (4.9.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a4e544c3-251e-4737-938c-9bc44d938952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torch==1.9\n",
      "  Downloading torch-1.9.0-cp38-cp38-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: typing-extensions in /home/viso/.local/lib/python3.8/site-packages (from torch==1.9) (4.9.0)\n",
      "Downloading torch-1.9.0-cp38-cp38-manylinux1_x86_64.whl (831.4 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m831.4/831.4 MB\u001b[0m \u001b[31m839.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torch\n",
      "\u001b[33m  WARNING: The scripts convert-caffe2-to-onnx and convert-onnx-to-caffe2 are installed in '/home/viso/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 0.7.2 requires torch==1.7.1, but you have torch 1.9.0 which is incompatible.\n",
      "torchvision 0.17.1 requires torch==2.2.1, but you have torch 1.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed torch-1.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "43771fb9-9712-47f9-9303-0524ca1b50af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in links: https://download.openmmlab.com/mmcv/dist/cu111/torch1.9/index.html\n",
      "Collecting mmcv==2.0.0rc4\n",
      "  Using cached https://download.openmmlab.com/mmcv/dist/cu111/torch1.9.0/mmcv-2.0.0rc4-cp38-cp38-manylinux1_x86_64.whl (50.7 MB)\n",
      "Requirement already satisfied: addict in /home/viso/.local/lib/python3.8/site-packages (from mmcv==2.0.0rc4) (2.4.0)\n",
      "Requirement already satisfied: mmengine>=0.2.0 in /home/viso/.local/lib/python3.8/site-packages (from mmcv==2.0.0rc4) (0.8.5)\n",
      "Requirement already satisfied: numpy in /home/viso/.local/lib/python3.8/site-packages (from mmcv==2.0.0rc4) (1.24.4)\n",
      "Requirement already satisfied: packaging in /home/viso/.local/lib/python3.8/site-packages (from mmcv==2.0.0rc4) (23.2)\n",
      "Requirement already satisfied: Pillow in /home/viso/.local/lib/python3.8/site-packages (from mmcv==2.0.0rc4) (9.5.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.8/site-packages (from mmcv==2.0.0rc4) (6.0)\n",
      "Requirement already satisfied: yapf in /home/viso/.local/lib/python3.8/site-packages (from mmcv==2.0.0rc4) (0.40.2)\n",
      "Requirement already satisfied: opencv-python>=3 in /home/viso/.local/lib/python3.8/site-packages (from mmcv==2.0.0rc4) (4.8.1.78)\n",
      "Requirement already satisfied: matplotlib in /home/viso/.local/lib/python3.8/site-packages (from mmengine>=0.2.0->mmcv==2.0.0rc4) (3.7.3)\n",
      "Requirement already satisfied: rich in /home/viso/.local/lib/python3.8/site-packages (from mmengine>=0.2.0->mmcv==2.0.0rc4) (13.4.2)\n",
      "Requirement already satisfied: termcolor in /home/viso/.local/lib/python3.8/site-packages (from mmengine>=0.2.0->mmcv==2.0.0rc4) (2.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=6.6.0 in /home/viso/.local/lib/python3.8/site-packages (from yapf->mmcv==2.0.0rc4) (6.8.0)\n",
      "Requirement already satisfied: platformdirs>=3.5.1 in /home/viso/.local/lib/python3.8/site-packages (from yapf->mmcv==2.0.0rc4) (3.11.0)\n",
      "Requirement already satisfied: tomli>=2.0.1 in /home/viso/.local/lib/python3.8/site-packages (from yapf->mmcv==2.0.0rc4) (2.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata>=6.6.0->yapf->mmcv==2.0.0rc4) (3.8.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/viso/.local/lib/python3.8/site-packages (from matplotlib->mmengine>=0.2.0->mmcv==2.0.0rc4) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/viso/.local/lib/python3.8/site-packages (from matplotlib->mmengine>=0.2.0->mmcv==2.0.0rc4) (0.10.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib->mmengine>=0.2.0->mmcv==2.0.0rc4) (4.37.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->mmengine>=0.2.0->mmcv==2.0.0rc4) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/viso/.local/lib/python3.8/site-packages (from matplotlib->mmengine>=0.2.0->mmcv==2.0.0rc4) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.8/site-packages (from matplotlib->mmengine>=0.2.0->mmcv==2.0.0rc4) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib->mmengine>=0.2.0->mmcv==2.0.0rc4) (5.9.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/viso/.local/lib/python3.8/site-packages (from rich->mmengine>=0.2.0->mmcv==2.0.0rc4) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/viso/.local/lib/python3.8/site-packages (from rich->mmengine>=0.2.0->mmcv==2.0.0rc4) (2.16.1)\n",
      "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in /home/viso/.local/lib/python3.8/site-packages (from rich->mmengine>=0.2.0->mmcv==2.0.0rc4) (4.9.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from cycler>=0.10->matplotlib->mmengine>=0.2.0->mmcv==2.0.0rc4) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/viso/.local/lib/python3.8/site-packages (from markdown-it-py>=2.2.0->rich->mmengine>=0.2.0->mmcv==2.0.0rc4) (0.1.2)\n",
      "Installing collected packages: mmcv\n",
      "Successfully installed mmcv-2.0.0rc4\n"
     ]
    }
   ],
   "source": [
    "!pip install mmcv==2.0.0rc4 -f https://download.openmmlab.com/mmcv/dist/cu111/torch1.9/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a467bd3-538c-4157-a9e5-b606a0ee6eae",
   "metadata": {},
   "source": [
    "# testing with the trained model with image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a4ebf468-1b33-4f27-81ba-cec6279cd94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rtmdet_s_syncbn_fast_8xb32-300e_coco\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Remove the last .py extension\n",
    "custom_config_file_path_without_extension = os.path.splitext(custom_config_file_path)[0]\n",
    "\n",
    "print(custom_config_file_path_without_extension)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "45cad64d-f2f6-46fc-b22f-fb5dbc612a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rtmdet_s_syncbn_fast_8xb32-300e_coco.py'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_config_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1c9a632b-c7da-4a04-82ab-c5337b1de485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/viso/mmyolo/configs/rtmdet/custom_config.py'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f427d975-1993-4add-8ae7-c36941559021",
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_option == '1' and test_type == '0':\n",
    "    %cd /home/viso/notebooks\n",
    "    %run ./test.ipynb\n",
    "    testing_image(MAX_EPOCHS, custom_config_file_path_base,random_middle_part,test_json_relpath,dummy_config)   \n",
    "    %cd /home/viso/mmyolo\n",
    "    import subprocess\n",
    "    command_line = f\"python tools/test.py \\\n",
    "        configs/{selected_model_type}/{custom_config_file_path} \\\n",
    "        work_dirs/{custom_config_file_path_without_extension}/epoch_{MAX_EPOCHS}.pth \\\n",
    "        --work-dir wk\"\n",
    "\n",
    "    # Run the command\n",
    "    result = subprocess.run(command_line, shell=True)\n",
    "\n",
    "    # Check the result\n",
    "    if result.returncode == 0:\n",
    "        print(\"Command executed successfully\")\n",
    "    else:\n",
    "        print(f\"Error executing the command. Return code: {result.returncode}\")\n",
    "        print(f\"Output: {result.stdout}\")\n",
    "        print(f\"Error: {result.stderr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7c4ca3-1d63-4f86-ad8c-6ebc6e178617",
   "metadata": {},
   "source": [
    "# testing with the trained model with video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "59cdaab8-d3ba-402d-a51f-0d4b93f62e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_option == '1' and test_type == '1':\n",
    "    %cd /home/viso/notebooks\n",
    "    %run ./test.ipynb\n",
    "    testing_video(MAX_EPOCHS, custom_config_file_path_base,video_path,output_path,custom_config_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b40a503-1f99-437c-8434-a365ef35df44",
   "metadata": {},
   "source": [
    "# Quantize the model (convert the pytorch model into tensorrt model )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c8bc9ec8-bf24-4859-be3a-f301f89b5f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "77ab195c-8aa4-4145-bd7b-2d0d6a229afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if quantize_option == '1':\n",
    "    %cd /home/viso/notebooks\n",
    "    %run ./quantize.ipynb\n",
    "    quantize(quantize_option, quantize_bits, batch_size_choice)\n",
    "    import subprocess\n",
    "    %cd /home/viso/github/\n",
    "    command_line = f\"python mmdeploy/tools/deploy.py \\\n",
    "    /home/viso/mmyolo/configs/deploy/detection_tensorrt_dynamic-192x192-960x960.py \\\n",
    "    {config_file_path} \\\n",
    "    ../mmyolo/work_dirs/{custom_config_file_path_without_extension}/epoch_{MAX_EPOCHS}.pth \\\n",
    "    /home/viso/datasets/train_val_test/val_data/1_013.jpg \\\n",
    "    --work-dir mmdeploy_model/{custom_config_file_path}_tensorrt \\\n",
    "    --device cuda \\\n",
    "    --dump-info\"\n",
    "\n",
    "    # Run the command\n",
    "    result = subprocess.run(command_line, shell=True)\n",
    "\n",
    "    # Check the result\n",
    "    if result.returncode == 0:\n",
    "        print(\"Command executed successfully\")\n",
    "    else:\n",
    "        print(f\"Error executing the command. Return code: {result.returncode}\")\n",
    "        print(f\"Output: {result.stdout}\")\n",
    "        print(f\"Error: {result.stderr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6acc9e2-9126-4612-b188-684bf91c0682",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
